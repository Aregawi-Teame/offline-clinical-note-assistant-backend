{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Clinical Note Assistant - Google Colab Setup\n",
    "\n",
    "This notebook sets up and runs the FastAPI backend on Google Colab with GPU support.\n",
    "\n",
    "## Prerequisites\n",
    "- Google Colab Pro (recommended) for GPU access\n",
    "- HuggingFace token (if model requires authentication)\n",
    "\n",
    "## Step 1: Enable GPU Runtime\n",
    "\n",
    "**Go to: Runtime â†’ Change runtime type â†’ GPU (T4 or better)**\n",
    "\n",
    "Make sure to select GPU before running the cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required packages including FastAPI, PyTorch with CUDA, and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA available: False\n",
      "âš ï¸  CUDA not available. Make sure you selected GPU runtime!\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q fastapi uvicorn[standard] pydantic pydantic-settings transformers torch accelerate pyngrok requests psutil\n",
    "\n",
    "# Verify CUDA is available\n",
    "import torch\n",
    "print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"âœ… CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"âš ï¸  CUDA not available. Make sure you selected GPU runtime!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone from GitHub (Recommended)\n",
    "\n",
    "**Option A: Clone from GitHub (Recommended)**\n",
    "- If you've pushed your code to GitHub, clone it here\n",
    "- This is the easiest and most reliable method\n",
    "\n",
    "**Option B: Upload Files Manually**\n",
    "- Use Colab's file uploader (right sidebar â†’ ğŸ“) to upload your `app/` directory\n",
    "\n",
    "After cloning/uploading, your directory structure should include:\n",
    "```\n",
    "app/\n",
    "  â”œâ”€â”€ main.py\n",
    "  â”œâ”€â”€ api/\n",
    "  â”œâ”€â”€ core/\n",
    "  â”œâ”€â”€ services/\n",
    "  â”œâ”€â”€ schemas/\n",
    "  â”œâ”€â”€ templates/\n",
    "  â””â”€â”€ utils/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Clone from GitHub (Recommended)\n",
    "# Replace with your actual GitHub repository URL and uncomment below:\n",
    "# REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git\"\n",
    "# REPO_DIR = \"YOUR_REPO_NAME\"  # Directory name after cloning\n",
    "\n",
    "# Example (uncomment and customize):\n",
    "REPO_URL = \"https://github.com/Aregawi-Teame/offline-clinical-note-assistant-backend.git\"\n",
    "REPO_DIR = \"offline-clinical-note-assistant-backend\"\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# IMPORTANT: Reset to /content to ensure consistent path handling if cell is re-run\n",
    "# This prevents nesting (cloning inside an existing clone)\n",
    "if os.path.exists('/content'):\n",
    "    os.chdir('/content')\n",
    "\n",
    "# Define absolute path to ensure reliability\n",
    "REPO_PATH = os.path.abspath(REPO_DIR)\n",
    "\n",
    "# Check if repository directory already exists\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"ğŸ“ Repository '{REPO_DIR}' already exists.\")\n",
    "\n",
    "    # Check if it's a git repository\n",
    "    git_dir = os.path.join(REPO_PATH, '.git')\n",
    "    if os.path.exists(git_dir):\n",
    "        print(\"   Detected git repository. Pulling latest changes...\")\n",
    "\n",
    "        # Change to repository directory first\n",
    "        os.chdir(REPO_PATH)\n",
    "        print(f\"ğŸ“‚ Changed to: {os.getcwd()}\")\n",
    "\n",
    "        # Pull latest changes\n",
    "        result = subprocess.run(['git', 'pull'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… Successfully pulled latest changes\")\n",
    "            if result.stdout.strip():\n",
    "                print(result.stdout)\n",
    "            else:\n",
    "                print(\"   (Already up to date)\")\n",
    "        else:\n",
    "            print(\"âš ï¸  Could not pull changes\")\n",
    "            if result.stdout.strip():\n",
    "                print(f\"   Output: {result.stdout}\")\n",
    "            if result.stderr.strip():\n",
    "                print(f\"   Error: {result.stderr}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  '{REPO_DIR}' exists but is not a git repository\")\n",
    "        print(f\"   Removing it and cloning fresh...\")\n",
    "        shutil.rmtree(REPO_PATH)\n",
    "        # Clone fresh\n",
    "        result = subprocess.run(['git', 'clone', REPO_URL], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Successfully cloned repository\")\n",
    "            os.chdir(REPO_PATH)\n",
    "        else:\n",
    "            print(f\"âŒ Failed to clone repository: {result.stderr}\")\n",
    "\n",
    "    # Ensure we're in the repo directory\n",
    "    if os.getcwd() != REPO_PATH:\n",
    "        os.chdir(REPO_PATH)\n",
    "        print(f\"ğŸ“‚ Current directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"ğŸ“¥ Cloning repository from {REPO_URL}...\")\n",
    "    result = subprocess.run(['git', 'clone', REPO_URL], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(f\"âœ… Successfully cloned repository\")\n",
    "        os.chdir(REPO_PATH)\n",
    "        print(f\"ğŸ“‚ Changed to directory: {os.getcwd()}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to clone repository\")\n",
    "        if result.stdout.strip():\n",
    "            print(f\"   Output: {result.stdout}\")\n",
    "        if result.stderr.strip():\n",
    "            print(f\"   Error: {result.stderr}\")\n",
    "        print(\"   Make sure the repository URL is correct and publicly accessible\")\n",
    "\n",
    "# Option B: If not using GitHub, upload files manually via Colab's file uploader\n",
    "# Use the files tab (ğŸ“) in the left sidebar to upload your app/ directory\n",
    "\n",
    "# Verify app directory exists\n",
    "print(\"\\nğŸ” Verifying project structure...\")\n",
    "if os.path.exists('app'):\n",
    "    print(\"âœ… app/ directory found\")\n",
    "    app_contents = os.listdir('app')\n",
    "    print(f\"ğŸ“ Contents: {app_contents}\")\n",
    "    if 'main.py' in app_contents:\n",
    "        print(\"âœ… main.py found - project structure looks good!\")\n",
    "    else:\n",
    "        print(\"âš ï¸  main.py not found - check your directory structure\")\n",
    "else:\n",
    "    print(\"âš ï¸  app/ directory not found.\")\n",
    "    print(\"   Please either:\")\n",
    "    print(\"   1. Uncomment and customize REPO_URL above, then re-run this cell\")\n",
    "    print(\"   2. Upload files manually via Colab's file uploader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Environment\n",
    "\n",
    "Set environment variables for the application. Colab will automatically use CUDA when `DEVICE=auto`.\n",
    "\n",
    "### ğŸ”‘ HuggingFace Token (May be Required)\n",
    "\n",
    "Some models require authentication. If you see errors about \"not a valid model identifier\", you may need a HuggingFace token:\n",
    "\n",
    "1. **Get your token**: Go to https://huggingface.co/settings/tokens\n",
    "2. **Create a token** (with \"read\" permissions)\n",
    "3. **Set it in Cell 6** (see next cell) by uncommenting the `HUGGINGFACE_HUB_TOKEN` line\n",
    "\n",
    "**Note**: Not all models require authentication. Try without a token first, and add it if you get authentication errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "# Configuration - these will override .env file if present\n",
    "# IMPORTANT: Set these BEFORE importing app modules (Settings reads env vars at import time)\n",
    "os.environ['ENV'] = 'dev'  # Must be 'dev' or 'prod' - prevents validation errors\n",
    "os.environ['DEVICE'] = 'auto'  # Will auto-detect CUDA in Colab\n",
    "os.environ['MODEL_ID'] = 'google/medgemma-1.5-4b-it'  # or 'google/medgemma-1.5-4b-it'\n",
    "os.environ['DEMO_MODE'] = 'false'  # Set to 'true' for demo mode (no model needed)\n",
    "os.environ['MAX_NEW_TOKENS'] = '800'\n",
    "os.environ['TEMPERATURE'] = '0.2'\n",
    "os.environ['TOP_P'] = '0.9'\n",
    "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "\n",
    "# IMPORTANT: HuggingFace Token (Required for some models)\n",
    "# If you get \"not a valid model identifier\" errors, the model may require authentication\n",
    "# \n",
    "# Get your token:\n",
    "# 1. Go to: https://huggingface.co/settings/tokens\n",
    "# 2. Create a token with \"read\" permissions\n",
    "# 3. Uncomment the line below and paste your token\n",
    "#\n",
    "# os.environ['HUGGINGFACE_HUB_TOKEN'] = 'hf_your_token_here'\n",
    "# \n",
    "# Alternative: Login using HuggingFace CLI\n",
    "# !huggingface-cli login --token YOUR_TOKEN\n",
    "\n",
    "# Check if token is set\n",
    "hf_token = os.environ.get('HUGGINGFACE_HUB_TOKEN') or os.environ.get('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(f\"âœ… HuggingFace token is set (will be used for model authentication)\")\n",
    "else:\n",
    "    print(\"â„¹ï¸  HuggingFace token not set (may be required for some models)\")\n",
    "\n",
    "print(\"âœ… Environment configured\")\n",
    "print(f\"   ENV: {os.environ.get('ENV')} (must be 'dev' or 'prod')\")\n",
    "print(f\"   DEVICE: {os.environ.get('DEVICE')}\")\n",
    "print(f\"   MODEL_ID: {os.environ.get('MODEL_ID')} âš ï¸  Make sure this matches what you want!\")\n",
    "print(f\"   DEMO_MODE: {os.environ.get('DEMO_MODE')}\")\n",
    "\n",
    "# Verify the environment variable is set\n",
    "model_id_set = os.environ.get('MODEL_ID', 'NOT SET')\n",
    "print(f\"\\nğŸ“‹ MODEL_ID is set to: {model_id_set}\")\n",
    "if model_id_set == 'NOT SET':\n",
    "    print(\"âš ï¸  WARNING: MODEL_ID not set!\")\n",
    "elif 'medgemma-1.5-4b-it' in model_id_set or 'medgemma-4b-it' in model_id_set:\n",
    "    print(\"   Try: 'google/medgemma-2b' or 'google/medgemma-7b'\")\n",
    "    print(\"   Search for valid models: https://huggingface.co/models?search=medgemma\")\n",
    "else:\n",
    "    print(f\"âœ… MODEL_ID set to: {model_id_set}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ IMPORTANT: Run this cell BEFORE starting the server (Cell 10)\")\n",
    "print(\"   The server must be restarted if you change MODEL_ID after it's running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Authenticate ngrok (Recommended)\n",
    "\n",
    "ngrok requires authentication for reliable tunnels. You can use it without authentication, but authenticated sessions are more stable.\n",
    "\n",
    "### Get ngrok Auth Token\n",
    "\n",
    "1. **Sign up for free**: Go to https://dashboard.ngrok.com/signup\n",
    "2. **Get your token**: After signing up, go to https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "3. **Copy the token** (looks like: `2abc123def456ghi789jkl_1a2B3c4D5e6F7g8H9i0J`)\n",
    "\n",
    "### Authenticate in Colab\n",
    "\n",
    "Run the cell below with your auth token. If you skip this, ngrok will still work but may have limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INSTRUCTIONS: Paste Your ngrok Token Here\n",
    "# ========================================\n",
    "#\n",
    "# After getting your token from https://dashboard.ngrok.com/get-started/your-authtoken:\n",
    "# 1. Find the line below that says: # !ngrok config add-authtoken YOUR_NGROK_AUTH_TOKEN\n",
    "# 2. Remove the # at the start (uncomment it)\n",
    "# 3. Replace YOUR_NGROK_AUTH_TOKEN with your actual token (paste between quotes)\n",
    "# 4. It should look like: !ngrok config add-authtoken 2abc123def456ghi789jkl_1a2B3c4D5e6F\n",
    "# 5. Run this cell\n",
    "#\n",
    "# Example (after you paste your token):\n",
    "# !ngrok config add-authtoken 2abc123def456ghi789jkl_1a2B3c4D5e6F7g8H9i0J\n",
    "#\n",
    "# ========================================\n",
    "\n",
    "# ğŸ‘‡ PASTE YOUR TOKEN HERE ğŸ‘‡\n",
    "# Uncomment the line below and replace YOUR_NGROK_AUTH_TOKEN with your actual token:\n",
    "# !ngrok config add-authtoken YOUR_NGROK_AUTH_TOKEN\n",
    "\n",
    "# Option: Skip authentication (works but may have limitations)\n",
    "# If you skip authentication, you can still use ngrok but sessions may timeout sooner\n",
    "# Just leave the line above commented and run this cell\n",
    "\n",
    "print(\"â„¹ï¸  ngrok authentication status:\")\n",
    "try:\n",
    "    from pyngrok import ngrok\n",
    "    # Try to check if authenticated (this is approximate)\n",
    "    print(\"   Ready to authenticate\")\n",
    "    print(\"   âš ï¸  To authenticate: Uncomment the line above, paste your token, and re-run this cell\")\n",
    "except:\n",
    "    print(\"   Pyngrok not imported yet (will be imported in next cell)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostic: Check tokenizer and model token configuration\n",
    "# Run this if you get \"CUDA device-side assert\" errors\n",
    "\n",
    "import os\n",
    "os.environ.setdefault('ENV', 'dev')\n",
    "os.environ.setdefault('MODEL_ID', 'google/medgemma-1.5-4b-it')\n",
    "os.environ.setdefault('DEVICE', 'auto')\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    import torch\n",
    "    \n",
    "    model_id = os.environ.get('MODEL_ID')\n",
    "    hf_token = os.environ.get('HF_TOKEN')\n",
    "    \n",
    "    print(f\"ğŸ” Diagnosing tokenizer/model: {model_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"ğŸ“¥ Loading tokenizer...\")\n",
    "    if hf_token:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Check token IDs\n",
    "    print(\"ğŸ”¢ Token Configuration:\")\n",
    "    print(f\"   vocab_size: {tokenizer.vocab_size}\")\n",
    "    print(f\"   pad_token_id: {tokenizer.pad_token_id}\")\n",
    "    print(f\"   eos_token_id: {tokenizer.eos_token_id}\")\n",
    "    print(f\"   bos_token_id: {tokenizer.bos_token_id}\")\n",
    "    print(f\"   unk_token_id: {tokenizer.unk_token_id}\")\n",
    "    print()\n",
    "    \n",
    "    # Validate token IDs are within bounds\n",
    "    print(\"âœ… Validation:\")\n",
    "    issues = []\n",
    "    \n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        if tokenizer.pad_token_id >= tokenizer.vocab_size:\n",
    "            issues.append(f\"âš ï¸  pad_token_id ({tokenizer.pad_token_id}) >= vocab_size ({tokenizer.vocab_size})\")\n",
    "        else:\n",
    "            print(f\"   âœ… pad_token_id is valid (within vocab bounds)\")\n",
    "    else:\n",
    "        print(f\"   â„¹ï¸  pad_token_id is None (will be set during model loading)\")\n",
    "    \n",
    "    if tokenizer.eos_token_id is not None:\n",
    "        if tokenizer.eos_token_id >= tokenizer.vocab_size:\n",
    "            issues.append(f\"âš ï¸  eos_token_id ({tokenizer.eos_token_id}) >= vocab_size ({tokenizer.vocab_size})\")\n",
    "        else:\n",
    "            print(f\"   âœ… eos_token_id is valid (within vocab bounds)\")\n",
    "    else:\n",
    "        issues.append(\"âš ï¸  eos_token_id is None (this will cause issues)\")\n",
    "    \n",
    "    if tokenizer.bos_token_id is not None:\n",
    "        if tokenizer.bos_token_id >= tokenizer.vocab_size:\n",
    "            issues.append(f\"âš ï¸  bos_token_id ({tokenizer.bos_token_id}) >= vocab_size ({tokenizer.vocab_size})\")\n",
    "        else:\n",
    "            print(f\"   âœ… bos_token_id is valid\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    if issues:\n",
    "        print(\"âŒ Issues Found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"   {issue}\")\n",
    "        print()\n",
    "        print(\"ğŸ’¡ Solution: This tokenizer/model has out-of-bounds token IDs.\")\n",
    "        print(\"   Try a different model like 'google/gemma-2b-it' or 'google/gemma-7b-it'\")\n",
    "    else:\n",
    "        print(\"âœ… All token IDs are valid - no configuration issues detected\")\n",
    "        print(\"   If you still get CUDA errors, try:\")\n",
    "        print(\"   1. Set DEVICE=cpu in Cell 6\")\n",
    "        print(\"   2. Restart the runtime and try again\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during diagnostic: {e}\")\n",
    "    print(\"   Make sure you've run Cell 6 (Configure Environment) first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up any existing ngrok tunnels (optional - run if you get tunnel errors)\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "\n",
    "print(\"ğŸ§¹ Cleaning up existing ngrok tunnels...\")\n",
    "\n",
    "# Disconnect all visible tunnels\n",
    "try:\n",
    "    tunnels = ngrok.get_tunnels()\n",
    "    for tunnel_info in tunnels:\n",
    "        tunnel_url = tunnel_info.public_url if hasattr(tunnel_info, 'public_url') else str(tunnel_info)\n",
    "        try:\n",
    "            ngrok.disconnect(tunnel_url)\n",
    "            print(f\"   âœ… Closed: {tunnel_url}\")\n",
    "        except:\n",
    "            pass\n",
    "    if tunnels:\n",
    "        time.sleep(2)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Kill any ngrok processes\n",
    "try:\n",
    "    import psutil\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        try:\n",
    "            if 'ngrok' in proc.info['name'].lower():\n",
    "                proc.kill()\n",
    "                print(f\"   âœ… Killed ngrok process (PID: {proc.pid})\")\n",
    "        except:\n",
    "            pass\n",
    "except ImportError:\n",
    "    try:\n",
    "        import subprocess\n",
    "        subprocess.run(['pkill', '-9', '-f', 'ngrok'], capture_output=True, timeout=2)\n",
    "    except:\n",
    "        pass\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"âœ… Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start FastAPI Server with ngrok\n",
    "\n",
    "This will start the FastAPI server and create a public URL using ngrok. The server will run in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import uvicorn\n",
    "import threading\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# IMPORTANT: Verify MODEL_ID is set correctly BEFORE starting server\n",
    "# The Settings class reads environment variables at import time (when server starts)\n",
    "# So make sure Cell 6 (Configure Environment) was run FIRST\n",
    "print(\"ğŸ” Verifying configuration before starting server...\")\n",
    "model_id_env = os.environ.get('MODEL_ID', 'NOT SET')\n",
    "print(f\"   Environment MODEL_ID: {model_id_env}\")\n",
    "\n",
    "if model_id_env == 'NOT SET':\n",
    "    print(\"âš ï¸  WARNING: MODEL_ID not set! Run Cell 6 (Configure Environment) first!\")\n",
    "elif 'medgemma-1.5-4b-it' in model_id_env or 'medgemma-4b-it' in model_id_env:\n",
    "    print(f\"âš ï¸  WARNING: MODEL_ID '{model_id_env}' may not be valid!\")\n",
    "    print(\"   Try: 'google/medgemma-2b' or 'google/medgemma-7b'\")\n",
    "    print(\"   Check: https://huggingface.co/models?search=medgemma\")\n",
    "else:\n",
    "    print(f\"âœ… MODEL_ID is set to: {model_id_env}\")\n",
    "    print(f\"   (If you get model loading errors, verify this model exists on HuggingFace)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Start ngrok tunnel\n",
    "print(\"\\nğŸš€ Starting ngrok tunnel...\")\n",
    "try:\n",
    "    tunnel = ngrok.connect(8000)\n",
    "except Exception as e:\n",
    "    error_str = str(e)\n",
    "    if \"ERR_NGROK_334\" in error_str or \"already online\" in error_str:\n",
    "        print(\"\\nâŒ Error: A tunnel endpoint is already in use\")\n",
    "        print(\"   Solutions:\")\n",
    "        print(\"   1. Run the 'Kill ngrok tunnels' cell below, then re-run this cell\")\n",
    "        print(\"   2. Go to https://dashboard.ngrok.com/status/tunnels and close it manually\")\n",
    "        print(\"   3. Restart runtime (Runtime â†’ Restart runtime)\")\n",
    "        raise\n",
    "    elif \"ERR_NGROK_324\" in error_str or \"limit\" in error_str.lower():\n",
    "        print(\"\\nâŒ Error: Too many tunnels (free tier limit: 5)\")\n",
    "        print(\"   Go to https://dashboard.ngrok.com/status/tunnels and close unnecessary tunnels\")\n",
    "        raise\n",
    "    else:\n",
    "        raise\n",
    "# Extract the public URL string from the tunnel object\n",
    "# The tunnel object string looks like: NgrokTunnel: \"https://xxx.ngrok-free.dev\" -> \"http://localhost:8000\"\n",
    "if hasattr(tunnel, 'public_url'):\n",
    "    public_url = tunnel.public_url\n",
    "elif hasattr(tunnel, 'data') and 'public_url' in tunnel.data:\n",
    "    public_url = tunnel.data['public_url']\n",
    "else:\n",
    "    # Extract URL from string representation\n",
    "    tunnel_str = str(tunnel)\n",
    "    import re\n",
    "    url_match = re.search(r'\"(https://[^\"]+)\"', tunnel_str)\n",
    "    if url_match:\n",
    "        public_url = url_match.group(1)\n",
    "    else:\n",
    "        public_url = tunnel_str  # Fallback\n",
    "\n",
    "print(f\"ğŸŒ Public API URL: {public_url}\")\n",
    "print(f\"ğŸ“š API Docs: {public_url}/api/v1/docs\")\n",
    "print(f\"ğŸ” Health Check: {public_url}/api/v1/health\")\n",
    "print()\n",
    "print(\"â³ Starting server... (this may take 30-60 seconds on first run)\")\n",
    "\n",
    "# Start FastAPI server in background thread\n",
    "def run_server():\n",
    "    try:\n",
    "        uvicorn.run(\n",
    "            \"app.main:app\",\n",
    "            host=\"0.0.0.0\",\n",
    "            port=8000,\n",
    "            log_level=\"info\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server error: {e}\")\n",
    "\n",
    "# Start server thread\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start and model to load\n",
    "print(\"Waiting for server to initialize...\")\n",
    "time.sleep(10)  # Initial wait\n",
    "\n",
    "# Try to check health\n",
    "max_retries = 12\n",
    "for i in range(max_retries):\n",
    "    try:\n",
    "        health_url = f\"{public_url}/api/v1/health\"\n",
    "        response = requests.get(health_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            print(\"\\nâœ… Server is ready!\")\n",
    "            print(f\"Response: {response.json()}\")\n",
    "            break\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if i < max_retries - 1:\n",
    "            print(f\"Waiting... ({i+1}/{max_retries}) - Server may still be loading model\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Health check timed out after {max_retries * 5} seconds\")\n",
    "            print(f\"   Try accessing manually: {public_url}/api/v1/health\")\n",
    "            print(f\"   Server is running - model may still be loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test the API\n",
    "\n",
    "Test the health endpoint and generate a clinical note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Health check\n",
    "# Note: public_url should be set from the previous cell\n",
    "# If you get an error, make sure you ran the \"Start FastAPI Server\" cell first\n",
    "try:\n",
    "    health_url = f\"{public_url}/api/v1/health\"\n",
    "    \n",
    "    # Try with SSL verification first, retry without if SSL error\n",
    "    response = None\n",
    "    for attempt in [True, False]:  # First with SSL, then without\n",
    "        try:\n",
    "            response = requests.get(health_url, timeout=10, verify=attempt)\n",
    "            break\n",
    "        except requests.exceptions.SSLError:\n",
    "            if attempt:  # First attempt failed, try without SSL\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            raise\n",
    "    \n",
    "    if response:\n",
    "        print(\"ğŸ“Š Health Check:\")\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(\"âŒ Failed to get health check response\")\n",
    "        \n",
    "except NameError:\n",
    "    print(\"âŒ Error: public_url not found. Please run the 'Start FastAPI Server' cell first.\")\n",
    "except requests.exceptions.SSLError as e:\n",
    "    print(f\"âŒ SSL Error: {e}\")\n",
    "    print(\"   ğŸ’¡ This is common with ngrok free tier. The API test cell handles this automatically.\")\n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"âŒ Connection Error: {e}\")\n",
    "    print(\"   ğŸ’¡ Check if the server is running. Try restarting the 'Start FastAPI Server' cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(f\"   Try accessing: {public_url}/api/v1/health manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a SOAP note\n",
    "url = f\"{public_url}/api/v1/generate/\"\n",
    "payload = {\n",
    "    \"task\": \"SOAP\",\n",
    "    \"notes\": \"Patient presents with chest pain. 45-year-old male with history of hypertension. Blood pressure 140/90, heart rate regular at 72 bpm.\",\n",
    "    \"options\": {\n",
    "        \"maxTokens\": 800,\n",
    "        \"temperature\": 0.2,\n",
    "        \"topP\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ Generating SOAP note...\")\n",
    "print(f\"   Notes: {payload['notes'][:60]}...\")\n",
    "print(f\"   â³ This may take 2-5 minutes on T4 GPU for {payload['options']['maxTokens']} tokens...\")\n",
    "print(f\"   ğŸ’¡ Tip: Reduce 'maxTokens' to 400-600 for faster testing\")\n",
    "print()\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Try with SSL verification first, then without if SSL error occurs\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        # First attempt: with SSL verification\n",
    "        verify_ssl = attempt == 0\n",
    "        \n",
    "        # Increased timeout for model generation (can take 3-5 minutes on T4 GPU)\n",
    "        response = requests.post(\n",
    "            url, \n",
    "            json=payload, \n",
    "            timeout=300,\n",
    "            verify=verify_ssl\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            print(\"âœ… Generation successful!\")\n",
    "            print(f\"\\nğŸ“ Generated Note ({result['task']}):\")\n",
    "            print(f\"\\n{result['output']}\")\n",
    "            print(f\"\\nğŸ“Š Metadata:\")\n",
    "            print(f\"   Model: {result['model']}\")\n",
    "            print(f\"   Latency: {result['latencyMs']:.2f} ms\")\n",
    "            print(f\"   Request ID: {result.get('requestId', 'N/A')}\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"âŒ Error {response.status_code}:\")\n",
    "            try:\n",
    "                print(json.dumps(response.json(), indent=2))\n",
    "            except:\n",
    "                print(response.text)\n",
    "            break\n",
    "            \n",
    "    except requests.exceptions.SSLError as e:\n",
    "        if attempt == 0:\n",
    "            print(f\"âš ï¸  SSL error on attempt {attempt + 1}: {str(e)[:100]}...\")\n",
    "            print(\"   Retrying without SSL verification (ngrok free tier sometimes has SSL issues)...\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"âŒ SSL error persisted after retries: {e}\")\n",
    "            print(\"\\nğŸ’¡ Troubleshooting SSL errors:\")\n",
    "            print(\"   1. Restart the ngrok tunnel (kill and restart the 'Start FastAPI Server' cell)\")\n",
    "            print(\"   2. Wait a few seconds and try again (ngrok may need time to stabilize)\")\n",
    "            print(\"   3. Check if the server is still running (try the health check cell)\")\n",
    "            print(\"   4. If using ngrok free tier, consider upgrading or authenticating ngrok\")\n",
    "            break\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"â±ï¸  Request timed out after 5 minutes.\")\n",
    "        print(\"   ğŸ’¡ Try reducing 'maxTokens' to 400-600 for faster generation\")\n",
    "        print(\"   ğŸ’¡ Or wait longer - generation can take 5+ minutes on T4 GPU for 800 tokens\")\n",
    "        break\n",
    "        \n",
    "    except requests.exceptions.ConnectionError as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"âš ï¸  Connection error on attempt {attempt + 1}: {str(e)[:100]}...\")\n",
    "            print(f\"   Retrying in 2 seconds... ({attempt + 2}/{max_retries})\")\n",
    "            import time\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"âŒ Connection error after {max_retries} attempts: {e}\")\n",
    "            print(\"\\nğŸ’¡ Troubleshooting connection errors:\")\n",
    "            print(\"   1. Check if the server is running (try the health check cell)\")\n",
    "            print(\"   2. Verify the ngrok URL is correct (check the 'Start FastAPI Server' cell output)\")\n",
    "            print(\"   3. The ngrok tunnel may have disconnected - restart it\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_type = type(e).__name__\n",
    "        if \"SSL\" in error_type or \"ssl\" in str(e).lower():\n",
    "            if attempt == 0:\n",
    "                print(f\"âš ï¸  SSL-related error: {str(e)[:100]}...\")\n",
    "                print(\"   Retrying without SSL verification...\")\n",
    "                continue\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "### Session Management\n",
    "- **Free Colab**: Sessions timeout after ~12 hours\n",
    "- **Colab Pro**: Up to 24 hours (with idle timeout)\n",
    "- Keep cells running or reconnect ngrok if session restarts\n",
    "\n",
    "### GPU Access\n",
    "- Colab Pro provides T4 GPU (sometimes A100)\n",
    "- `DEVICE=auto` will automatically use CUDA when GPU is available\n",
    "- First request is slower (model loading ~30-60 seconds)\n",
    "\n",
    "### ngrok Authentication\n",
    "- **Get free token**: Sign up at https://dashboard.ngrok.com/signup\n",
    "- **Authenticate**: Use your token in Step 4 (cell above) before starting server\n",
    "- **Why authenticate**: Longer tunnels, better stability, no random disconnects\n",
    "- **Without auth**: Still works but may have shorter timeouts\n",
    "\n",
    "### ngrok URL\n",
    "- Free tier: URL changes on restart\n",
    "- Paid tier: Can use fixed domain\n",
    "- Save your public URL if you need to use it elsewhere\n",
    "\n",
    "### Using the API from Outside Colab\n",
    "\n",
    "Your API is now accessible via the public URL. Example curl command:\n",
    "```bash\n",
    "curl -X POST \"YOUR_NGROK_URL/api/v1/generate/\" \\\\\n",
    "  -H \"Content-Type: application/json\" \\\\\n",
    "  -d '{\"task\": \"SOAP\", \"notes\": \"Your clinical notes here\"}'\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
